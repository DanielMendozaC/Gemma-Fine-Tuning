{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Spanish Language Adaptation of Gemma 2: A Comprehensive Approach\n","\n","## Introduction\n","This notebook demonstrates the fine-tuning of Gemma 2 for Spanish language understanding and generation. Spanish, spoken by over 580 million people worldwide, offers significant opportunities for AI development due to its diverse dialects, cultural richness, and global influence.\n","\n","### Why Spanish?\n","- Widespread usage across multiple continents\n","- Rich linguistic structure with regional variations\n","- High demand for translation, cultural content, and technical documentation\n","- Underrepresented domains in AI, such as regional dialects and cultural idioms\n","\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["### Select the Runtime for Kaggle\n","\n","To successfully run this notebook, ensure your Kaggle runtime is configured with sufficient resources to handle the Gemma model. Follow these steps to enable a GPU accelerator:\n","\n","1. Open the **Settings** panel on the right-hand side of the Kaggle notebook.\n","2. Locate the **Accelerator** section and select **GPU (NVIDIA T4)** from the dropdown menu.\n","3. Confirm the settings and restart the notebook to apply the changes.\n","\n","Configuring the runtime with a GPU will ensure efficient execution of the fine-tuning and inference processes.\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["## Install dependencies\n","Install Keras, KerasNLP, and other dependencies."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:21:47.389612Z","iopub.status.busy":"2025-01-25T02:21:47.389217Z","iopub.status.idle":"2025-01-25T02:21:52.054717Z","shell.execute_reply":"2025-01-25T02:21:52.053708Z","shell.execute_reply.started":"2025-01-25T02:21:47.389579Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:21:52.056297Z","iopub.status.busy":"2025-01-25T02:21:52.056015Z","iopub.status.idle":"2025-01-25T02:22:00.810393Z","shell.execute_reply":"2025-01-25T02:22:00.809289Z","shell.execute_reply.started":"2025-01-25T02:21:52.056276Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h"]}],"source":["# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n","!pip install -q -U keras-nlp\n","!pip install -q -U \"keras>=3\""]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:22:00.812418Z","iopub.status.busy":"2025-01-25T02:22:00.812125Z","iopub.status.idle":"2025-01-25T02:22:05.122849Z","shell.execute_reply":"2025-01-25T02:22:05.121982Z","shell.execute_reply.started":"2025-01-25T02:22:00.812396Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Collecting peft\n","  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n","Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n","  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: huggingface-hub, peft\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.24.7\n","    Uninstalling huggingface-hub-0.24.7:\n","      Successfully uninstalled huggingface-hub-0.24.7\n","Successfully installed huggingface-hub-0.27.1 peft-0.14.0\n"]}],"source":["!pip install transformers peft sentencepiece"]},{"cell_type":"markdown","metadata":{},"source":["## Import packages"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:22:05.124849Z","iopub.status.busy":"2025-01-25T02:22:05.124524Z","iopub.status.idle":"2025-01-25T02:22:14.133422Z","shell.execute_reply":"2025-01-25T02:22:14.132728Z","shell.execute_reply.started":"2025-01-25T02:22:05.124817Z"},"trusted":true},"outputs":[],"source":["import os\n","import keras\n","import keras_nlp\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["## Choose a Backend  \n","Keras is a user-friendly deep learning API that works seamlessly with multiple frameworks. With Keras 3, you can build and run workflows using TensorFlow, JAX, or PyTorch as the backend.  \n","\n","In this tutorial, we’ll set up JAX as the backend.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:22:14.134856Z","iopub.status.busy":"2025-01-25T02:22:14.134242Z","iopub.status.idle":"2025-01-25T02:22:14.138591Z","shell.execute_reply":"2025-01-25T02:22:14.137694Z","shell.execute_reply.started":"2025-01-25T02:22:14.134823Z"},"trusted":true},"outputs":[],"source":["os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n","# Avoid memory fragmentation on JAX backend.\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""]},{"cell_type":"markdown","metadata":{},"source":["\n","## 1. Dataset Creation and Curation\n","\n","### 1.1 Data Sources\n","The dataset was curated specifically for the task of translating English to Spanish, focusing on diverse contexts and linguistic nuances. The primary sources include:\n","- **Manually Created Pairs**: Translation pairs generated to ensure high-quality, accurate translations.\n","- **Synthetic Data**: Augmented using language models (e.g., GPT) for generating translations to expand the dataset.\n","- **Public Translation Datasets**: Leveraging publicly available datasets with English-to-Spanish translation examples.\n","\n","### 1.2 Data Processing Pipeline\n","- **Formatting**: Converting data into the required instruction-response format for fine-tuning Gemma.\n","---"]},{"cell_type":"markdown","metadata":{},"source":["It is important to mention that in this notebook we are going to limit our training process to first 1000 rows of the dataset because if we use all the data it can take a lot of time."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:22:14.140292Z","iopub.status.busy":"2025-01-25T02:22:14.139927Z","iopub.status.idle":"2025-01-25T02:22:14.563642Z","shell.execute_reply":"2025-01-25T02:22:14.562829Z","shell.execute_reply.started":"2025-01-25T02:22:14.140259Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>english</th>\n","      <th>spanish</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Tom released Mary.</td>\n","      <td>Tom soltó a Mary.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>I already saw it.</td>\n","      <td>Ya lo vi.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>This is how I solved the problem.</td>\n","      <td>Así es como resolví el problema.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>You must be worn out after working all day.</td>\n","      <td>Sin duda debes de estar agotado después de tra...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>I'm not your girlfriend.</td>\n","      <td>No soy tu novia.</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>995</td>\n","      <td>They finally made peace with the enemy.</td>\n","      <td>Al final hicieron las paces con el enemigo.</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>996</td>\n","      <td>My father often falls asleep while watching TV.</td>\n","      <td>Mi padre suele quedarse dormido viendo la tele...</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>997</td>\n","      <td>How long are you going to stay here?</td>\n","      <td>¿Cuánto tiempo vas a quedarte aquí?</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>998</td>\n","      <td>I don't think people should make a mountain of...</td>\n","      <td>No creo que la gente deba hacer una montaña de...</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>999</td>\n","      <td>I forgot to attach a stamp to the envelope.</td>\n","      <td>Olvidé pegar un sello en el sobre.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 3 columns</p>\n","</div>"],"text/plain":["     Unnamed: 0                                            english  \\\n","0             0                                 Tom released Mary.   \n","1             1                                  I already saw it.   \n","2             2                  This is how I solved the problem.   \n","3             3        You must be worn out after working all day.   \n","4             4                           I'm not your girlfriend.   \n","..          ...                                                ...   \n","995         995            They finally made peace with the enemy.   \n","996         996    My father often falls asleep while watching TV.   \n","997         997               How long are you going to stay here?   \n","998         998  I don't think people should make a mountain of...   \n","999         999        I forgot to attach a stamp to the envelope.   \n","\n","                                               spanish  \n","0                                    Tom soltó a Mary.  \n","1                                            Ya lo vi.  \n","2                     Así es como resolví el problema.  \n","3    Sin duda debes de estar agotado después de tra...  \n","4                                     No soy tu novia.  \n","..                                                 ...  \n","995        Al final hicieron las paces con el enemigo.  \n","996  Mi padre suele quedarse dormido viendo la tele...  \n","997                ¿Cuánto tiempo vas a quedarte aquí?  \n","998  No creo que la gente deba hacer una montaña de...  \n","999                 Olvidé pegar un sello en el sobre.  \n","\n","[1000 rows x 3 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["spanish_dataset_path = '/kaggle/input/spanish-dataset/Spanish_Dataset.csv'\n","df = pd.read_csv(spanish_dataset_path)[:1000]\n","df"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:22:14.564706Z","iopub.status.busy":"2025-01-25T02:22:14.564425Z","iopub.status.idle":"2025-01-25T02:22:14.619499Z","shell.execute_reply":"2025-01-25T02:22:14.618513Z","shell.execute_reply.started":"2025-01-25T02:22:14.564682Z"},"trusted":true},"outputs":[],"source":["data = []\n","\n","for _, row in df.iterrows():\n","    instruction = f\"Translate to Spanish: {row['english']}\"\n","    response = row['spanish']\n","\n","    # Format the English and Spanish phrases using the template\n","    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n","    data.append(template.format(instruction=instruction, response=response))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 2 Evaluating the Gemma Base Model\n","\n","Before proceeding with fine-tuning, we will evaluate the capabilities of the **Gemma 2B base model** in generating Spanish text. This step establishes a baseline for comparison and helps us understand the improvements achieved through fine-tuning.\n","\n","### Objectives:\n","- Test the base model's Spanish fluency and coherence with sample prompts.\n","- Identify areas where the model struggles, such as cultural nuances, grammar, or translation accuracy.\n","\n","### Evaluation Method:\n","- Provide a set of Spanish-focused prompts covering conversational, cultural, and technical scenarios.\n","- Analyze the outputs for fluency, grammar, and contextual relevance.\n","\n","### Example Prompts:\n","1. \"Translate to Spanish: Hello, how are you?\"\n","2. \"What is the weather like in Madrid?\"\n","3. \"Explain the cultural significance of Día de los Muertos.\"\n","4. \"Translate to Spanish: I would like a cup of coffee, please.\"\n","\n","This evaluation will serve as a reference point to measure the improvements achieved after fine-tuning the model for Spanish-specific tasks.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:22:14.620723Z","iopub.status.busy":"2025-01-25T02:22:14.620460Z","iopub.status.idle":"2025-01-25T02:23:17.591097Z","shell.execute_reply":"2025-01-25T02:23:17.590309Z","shell.execute_reply.started":"2025-01-25T02:22:14.620702Z"},"trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["gemma_lm_base = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n","gemma_lm_base.summary()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:27:49.862208Z","iopub.status.busy":"2025-01-25T02:27:49.861836Z","iopub.status.idle":"2025-01-25T02:28:27.772864Z","shell.execute_reply":"2025-01-25T02:28:27.771954Z","shell.execute_reply.started":"2025-01-25T02:27:49.862181Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"Instruction:\\nTranslate to Spanish: Hello, how are you?\\n\\nResponse:\\nHola, ¿cómo estás?\\n\\nInstruction:\\nTranslate to Spanish: I'm fine, thank you.\\n\\nResponse:\\nEstoy bien, gracias.\\n\\nInstruction:\\nTranslate to Spanish: How are you?\\n\\nResponse:\\n¿Cómo estás?\\n\\nInstruction:\\nTranslate to Spanish: I'm fine, thank you.\\n\\nResponse:\\nEstoy bien, gracias.\\n\\nInstruction:\\nTranslate to Spanish: How are you?\\n\\nResponse:\\n¿Cómo estás?\\n\\nInstruction:\\nTranslate to Spanish: I'm fine, thank you.\\n\\nResponse:\\nEstoy bien, gracias.\\n\\nInstruction:\\nTranslate to Spanish: How are you?\\n\\nResponse:\\n¿Cómo estás?\\n\\nInstruction:\\nTranslate to Spanish: I'm fine, thank you.\\n\\nResponse:\\nEstoy bien, gracias.\\n\\nInstruction:\\nTranslate to Spanish: How are you?\\n\\nResponse:\\n¿Cómo estás?\\n\\nInstruction:\\nTranslate to Spanish: I'm fine, thank you.\\n\\nResponse:\\nEstoy bien, gracias.\\n\\nInstruction:\\nTranslate to Spanish: How are you?\\n\\nResponse:\\n¿Cómo estás?\\n\\nInstruction:\\nTranslate to Spanish\""]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["prompt1 = template.format(\n","    instruction=\"Translate to Spanish: Hello, how are you?\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm_base.generate(prompt1, max_length=256))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:28:27.774573Z","iopub.status.busy":"2025-01-25T02:28:27.774225Z","iopub.status.idle":"2025-01-25T02:28:34.493426Z","shell.execute_reply":"2025-01-25T02:28:34.492441Z","shell.execute_reply.started":"2025-01-25T02:28:27.774535Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'Instruction:\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\nWhat is the weather like in Madrid?\\n\\nResponse:\\nIt is very hot.\\n\\n'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["prompt2 = template.format(\n","    instruction=\"Translate to Spanish: What is the weather like in Madrid?\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm_base.generate(prompt2, max_length=256))"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:28:34.495599Z","iopub.status.busy":"2025-01-25T02:28:34.495377Z","iopub.status.idle":"2025-01-25T02:28:41.131892Z","shell.execute_reply":"2025-01-25T02:28:41.131159Z","shell.execute_reply.started":"2025-01-25T02:28:34.495580Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'Instruction:\\nExplain the cultural significance of Día de los Muertos.\\n\\nResponse:\\nThe cultural significance of Día de los Muertos is to celebrate the lives of the deceased. It is a time to remember and honor the lives of loved ones who have passed away. The holiday is celebrated in Mexico and other parts of Latin America, and it is a time for families to come together and share stories and memories of their loved ones. The holiday is also a time for people to decorate their homes and graves with flowers, candles, and other symbols of life.\\n\\nThe holiday is also a time for people to reflect on the lives of their loved ones and to remember the good times they shared. It is a time for people to come together and share stories and memories of their loved ones. The holiday is also a time for people to decorate their homes and graves with flowers, candles, and other symbols of life.\\n\\nThe holiday is also a time for people to reflect on the lives of their loved ones and to remember the good times they shared. It is a time for people to come together and share stories and memories of their loved ones. The holiday is also a time for people to decorate their homes and graves with flowers, candles, and other symbols of life.\\n\\n'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["prompt3 = template.format(\n","    instruction=\"Translate to Spanish: Explicar el significado cultural del Día de los Muertos.\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm_base.generate(prompt3, max_length=256))"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:28:41.133424Z","iopub.status.busy":"2025-01-25T02:28:41.133164Z","iopub.status.idle":"2025-01-25T02:28:47.687552Z","shell.execute_reply":"2025-01-25T02:28:47.686735Z","shell.execute_reply.started":"2025-01-25T02:28:41.133402Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'Instruction:\\nTranslate to Spanish: I would like a cup of coffee, please.\\n\\nResponse:\\nMe gustaría una taza de café, por favor.\\n\\nExplanation:\\nThe phrase \"I would like a cup of coffee, please\" is a request for a cup of coffee. In Spanish, the phrase is translated as \"Me gustaría una taza de café, por favor.\" The phrase \"por favor\" is used to express politeness and to indicate that the request is made politely.\\n\\nIn this case, the phrase \"Me gustaría una taza de café, por favor\" is used to make a request for a cup of coffee. The phrase \"por favor\" is used to express politeness and to indicate that the request is made politely.\\n\\nThe phrase \"Me gustaría\" means \"I would like\" and \"una taza de café\" means \"a cup of coffee.\" The phrase \"por favor\" means \"please\" and is used to express politeness.\\n\\nIn summary, the phrase \"Me gustaría una taza de café, por favor\" is a polite request for a cup of coffee. The phrase \"por favor\" is used to express politeness and to indicate that the request is made politely.\\n\\nI hope this helps!\\n\\nExplanation:\\n\\nI would like a cup'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["prompt4 = template.format(\n","    instruction=\"Translate to Spanish: I would like a cup of coffee, please.\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm_base.generate(prompt4, max_length=256))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## 3. Model Fine-Tuning Approach\n","\n","### 3.1 Technical Implementation\n","- **Base model**: Gemma 2B\n","- **Fine-tuning strategy**: LoRA (Low-Rank Adaptation) for efficiency and performance\n","- **Hyperparameter tuning**: Optimized for Spanish text, including conversational and cultural nuances\n","\n","### 3.2 Training Process\n","- Adaptation of pre-trained Gemma weights using LoRA\n","- Focused training on diverse Spanish datasets\n","- Batch size, learning rate, and sequence length carefully adjusted to balance performance and resource usage\n","\n","---"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:28:47.688836Z","iopub.status.busy":"2025-01-25T02:28:47.688531Z","iopub.status.idle":"2025-01-25T02:28:47.892458Z","shell.execute_reply":"2025-01-25T02:28:47.891785Z","shell.execute_reply.started":"2025-01-25T02:28:47.688802Z"},"trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n","└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n","│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n","│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n","└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"]},"metadata":{},"output_type":"display_data"}],"source":["gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n","\n","# Enable LoRA for the model and set the LoRA rank to 4.\n","gemma_lm.backbone.enable_lora(rank=4)\n","gemma_lm.summary()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T02:28:47.893620Z","iopub.status.busy":"2025-01-25T02:28:47.893275Z","iopub.status.idle":"2025-01-25T03:05:36.360122Z","shell.execute_reply":"2025-01-25T03:05:36.358856Z","shell.execute_reply.started":"2025-01-25T02:28:47.893588Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 426ms/step - loss: 0.2919 - sparse_categorical_accuracy: 0.5395\n","Epoch 2/5\n","\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 426ms/step - loss: 0.1611 - sparse_categorical_accuracy: 0.7512\n","Epoch 3/5\n","\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 426ms/step - loss: 0.1251 - sparse_categorical_accuracy: 0.7625\n","Epoch 4/5\n","\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 426ms/step - loss: 0.1071 - sparse_categorical_accuracy: 0.7891\n","Epoch 5/5\n","\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 426ms/step - loss: 0.1050 - sparse_categorical_accuracy: 0.7909\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7cfeac34ea70>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["gemma_lm.preprocessor.sequence_length = 256\n","\n","optimizer = keras.optimizers.AdamW(\n","    learning_rate=1e-5,\n","    weight_decay=0.005,\n",")\n","\n","optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n","\n","gemma_lm.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=optimizer,\n","    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",")\n","gemma_lm.fit(data, epochs=5, batch_size=1)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## 4. Inference and Evaluation\n","- Evaluation of the model’s fluency and coherence in Spanish.\n","- Testing across multiple domains, including conversational tasks, cultural content, and technical translations.\n","\n","---"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T03:18:11.858651Z","iopub.status.busy":"2025-01-25T03:18:11.858309Z","iopub.status.idle":"2025-01-25T03:18:12.113341Z","shell.execute_reply":"2025-01-25T03:18:12.112418Z","shell.execute_reply.started":"2025-01-25T03:18:11.858623Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Instruction:\n","Translate to Spanish: hello\n","\n","Response:\n","hola\n"]}],"source":["prompt1 = template.format(\n","    instruction=\"Translate to Spanish: hello\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm.generate(prompt1, max_length=256))"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T03:18:12.114671Z","iopub.status.busy":"2025-01-25T03:18:12.114430Z","iopub.status.idle":"2025-01-25T03:18:12.450982Z","shell.execute_reply":"2025-01-25T03:18:12.450221Z","shell.execute_reply.started":"2025-01-25T03:18:12.114649Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Instruction:\n","Translate to Spanish: how are you?\n","\n","Response:\n","¿Cómo estás?\n"]}],"source":["prompt2 = template.format(\n","    instruction=\"Translate to Spanish: how are you?\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm.generate(prompt2, max_length=256))"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T03:18:12.453015Z","iopub.status.busy":"2025-01-25T03:18:12.452740Z","iopub.status.idle":"2025-01-25T03:18:12.822269Z","shell.execute_reply":"2025-01-25T03:18:12.821344Z","shell.execute_reply.started":"2025-01-25T03:18:12.452991Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Instruction:\n","Translate to Spanish: I like to read books\n","\n","Response:\n","Me gusta leer libros.\n"]}],"source":["prompt3 = template.format(\n","    instruction=\"Translate to Spanish: I like to read books\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm.generate(prompt3, max_length=256))"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T03:18:12.823768Z","iopub.status.busy":"2025-01-25T03:18:12.823523Z","iopub.status.idle":"2025-01-25T03:18:13.126453Z","shell.execute_reply":"2025-01-25T03:18:13.125631Z","shell.execute_reply.started":"2025-01-25T03:18:12.823745Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Instruction:\n","Translate to Spanish: one two three\n","\n","Response:\n","uno dos tres\n"]}],"source":["prompt4 = template.format(\n","    instruction=\"Translate to Spanish: one two three\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm.generate(prompt4, max_length=256))"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T03:18:13.127510Z","iopub.status.busy":"2025-01-25T03:18:13.127275Z","iopub.status.idle":"2025-01-25T03:18:13.563288Z","shell.execute_reply":"2025-01-25T03:18:13.562519Z","shell.execute_reply.started":"2025-01-25T03:18:13.127489Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Instruction:\n","Translate to Spanish: The weather is very nice today\n","\n","Response:\n","El tiempo es muy bonito hoy.\n"]}],"source":["prompt5 = template.format(\n","    instruction=\"Translate to Spanish: The weather is very nice today\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm.generate(prompt5, max_length=256))"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T03:55:43.362996Z","iopub.status.busy":"2025-01-25T03:55:43.362681Z","iopub.status.idle":"2025-01-25T03:55:44.142862Z","shell.execute_reply":"2025-01-25T03:55:44.142036Z","shell.execute_reply.started":"2025-01-25T03:55:43.362975Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Instruction:\n","Translate to Spanish. Explain the cultural significance of Día de los Muertos.\n","\n","Response:\n","Traducir al español. Explique la importancia cultural de Día de los Muertos.\n"]}],"source":["prompt6 = template.format(\n","    instruction=\"Translate to Spanish. Explain the cultural significance of Día de los Muertos.\",\n","    response=\"\",\n",")\n","\n","print(gemma_lm.generate(prompt6, max_length=256))"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Uploading the Fine-Tuned Model to Kaggle\n","\n","After completing the evaluation process, we upload the final fine-tuned model to **Kaggle Models** to make it accessible for everyone. To handle the large model size, we save it temporarily in `/kaggle/tmp`, as the Kaggle notebook output directory has size limitations. This ensures seamless sharing and reproducibility of the fine-tuned model.\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T03:18:21.663830Z","iopub.status.busy":"2025-01-25T03:18:21.663530Z","iopub.status.idle":"2025-01-25T03:18:21.667352Z","shell.execute_reply":"2025-01-25T03:18:21.666473Z","shell.execute_reply.started":"2025-01-25T03:18:21.663792Z"},"trusted":true},"outputs":[],"source":["tmp_model_dir = \"/kaggle/tmp/gemma2_spa\" \n","preset_dir = \"gemma2_spa\"\n","os.makedirs(tmp_model_dir, exist_ok=True)\n","gemma_lm.save_to_preset(tmp_model_dir)\n","\n","print(f\"Model saved to: {tmp_model_dir}\")"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2025-01-25T03:18:21.668349Z","iopub.status.busy":"2025-01-25T03:18:21.668027Z","iopub.status.idle":"2025-01-25T03:18:21.683268Z","shell.execute_reply":"2025-01-25T03:18:21.682390Z","shell.execute_reply.started":"2025-01-25T03:18:21.668280Z"},"trusted":true},"outputs":[],"source":["import kagglehub\n","import keras_hub\n","if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n","    kagglehub.login()\n","\n","model_version = 1\n","kaggle_username = kagglehub.whoami()[\"username\"]\n","kaggle_uri = f\"kaggle://{kaggle_username}/gemma2/keras/{preset_dir}\"\n","keras_hub.upload_preset(kaggle_uri, tmp_model_dir)\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Conclusion 🎯\n","\n","This project demonstrates the successful fine-tuning of Gemma 2 for Spanish language translation. Through LoRA implementation and careful hyperparameter optimization, we achieved 79.09% accuracy on the training dataset. \n","\n","Key achievements include:\n","- Efficient model adaptation using LoRA\n","- Successful translation of basic phrases and greetings\n","- Reduced parameter count while maintaining performance\n","\n","The model provides a foundation for broader language adaptation efforts in the Gemma ecosystem. Future work could explore larger datasets, domain-specific vocabularies, and additional Spanish dialects.\n","\n","Code and model available on Kaggle for community use and improvement."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":9690815,"sourceId":85416,"sourceType":"competition"},{"datasetId":6525889,"sourceId":10547312,"sourceType":"datasetVersion"},{"modelId":78150,"modelInstanceId":72244,"sourceId":85984,"sourceType":"modelInstanceVersion"},{"modelId":218086,"modelInstanceId":196186,"sourceId":230075,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
